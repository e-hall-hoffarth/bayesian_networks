{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph \n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "import warnings\n",
    "from state_space_estimation.roles import roles\n",
    "from scipy.stats import multivariate_normal as mn\n",
    "%matplotlib inline\n",
    "\n",
    "tol = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dag():\n",
    "    \n",
    "    def __init__(self, m):\n",
    "        '''\n",
    "        m is a pandas DataFrame conaining an adjacency matrix\n",
    "        '''\n",
    "        self.m = m\n",
    "        self.nodes = self.m.columns.values\n",
    "        self.lags = np.array([n for n in self.nodes if '_1' in n])\n",
    "        if not all(self.m.columns.values == self.m.index.values):\n",
    "            raise ValueError('Invalid adjacency matrix (columns and rows are not the same)')\n",
    "        self.directed = self.directed()\n",
    "\n",
    "            \n",
    "    def parents(self, n):\n",
    "        '''\n",
    "        n is a node in the graph\n",
    "        '''\n",
    "        if n not in self.nodes:\n",
    "            raise ValueError('n is not in graph')\n",
    "        return self.m.loc[self.m.loc[:,n] != 0, n].index.values\n",
    "    \n",
    "    \n",
    "    def children(self, n):\n",
    "        '''\n",
    "        n is a node in the graph\n",
    "        '''\n",
    "        if n not in self.nodes:\n",
    "            raise ValueError('n is not in graph')\n",
    "        return self.m.loc[:,self.m.loc[n,:] != 0].columns.values\n",
    "    \n",
    "    \n",
    "    def directed(self):\n",
    "        return utils.is_dag(self.m.values)\n",
    "        \n",
    "    \n",
    "    def depth(self, n):\n",
    "        '''\n",
    "        n is a node in the graph\n",
    "        return the length of the shortest path to a root node\n",
    "        '''\n",
    "        if not self.directed:\n",
    "            raise ValueError('Cannot compute depth, graph is undirected')\n",
    "        if len(self.parents(n)) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 + max([self.depth(p) for p in self.parents(n)])\n",
    "    \n",
    "    \n",
    "    def root_nodes(self):\n",
    "        return np.array([n for n in self.nodes if len(self.parents(n)) == 0])\n",
    "        \n",
    "        \n",
    "    def isolated_nodes(self):\n",
    "        return np.array([n for n in d.nodes if (len(d.parents(n)) == 0) & (len(d.children(n)) == 0)])\n",
    "    \n",
    "    \n",
    "    def connected_roots(self):\n",
    "        return np.array([n for n in d.nodes if (len(d.parents(n)) == 0) & (len(d.children(n)) > 0)])\n",
    "    \n",
    "    \n",
    "    def structure(self):\n",
    "        M = self.m.copy()\n",
    "        M[M != 0] = 1\n",
    "        return M\n",
    "    \n",
    "    \n",
    "    def shd(self, d):\n",
    "        try:\n",
    "            return utils.count_accuracy(d.structure().values, self.structure().values)['shd']\n",
    "        except ValueError as e:\n",
    "            # Graph is not directed\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def impute(self, values):\n",
    "        na_nodes = values[values.isna()]\n",
    "        depth = 0\n",
    "        try:\n",
    "            max_depth = max([self.depth(n) for n in self.nodes])\n",
    "        except ValueError as e:\n",
    "            print('Cannot impute values as graph is not directed')\n",
    "            raise e\n",
    "        while depth <= max_depth:\n",
    "            for node in na_nodes.index:\n",
    "                if self.depth(node) == depth:\n",
    "                    values[node] = np.dot(self.m.loc[:,node], values.fillna(0))\n",
    "            depth += 1\n",
    "        return values\n",
    "\n",
    "\n",
    "    def calculate_irf(self, x_0, T=250, verbose=False):\n",
    "        if verbose:\n",
    "            print('Simulating irf...')\n",
    "        for lag in self.lags:\n",
    "            x_0[lag] = 0\n",
    "        irf = pd.DataFrame([self.impute(x_0)], columns=self.nodes)\n",
    "        for t in range(T-1):\n",
    "            if verbose:\n",
    "                print('Simulating t={} of {} ({}%)'.format(t+1, T, np.round(100 * ((t+1)/T), decimals=2)))\n",
    "            nr = pd.Series(np.full(len(self.nodes), np.nan), index=self.nodes)\n",
    "            for lag in self.lags:\n",
    "                nr[lag] = irf.iloc[-1,:][lag.rstrip('_1')]\n",
    "            nr = self.impute(nr)    \n",
    "            irf = irf.append(nr, ignore_index=True)\n",
    "        irf.loc[:,'t'] = range(T)\n",
    "        irf.set_index('t', inplace=True)\n",
    "        irf.drop(self.lags, axis=1, inplace=True)\n",
    "        return irf\n",
    "\n",
    "\n",
    "    def plot_irf(self, irf, layout=None):\n",
    "        if layout is None:\n",
    "            side = math.ceil(math.sqrt(len(irf.columns)))\n",
    "            layout = (side, side)\n",
    "        axes = irf.plot(subplots=True, layout=layout, \n",
    "                        color=\"black\", legend=False)\n",
    "        for ax, name in zip(axes.flatten(), irf.columns.values):\n",
    "            ax.axhline(y=0, color=\"red\")\n",
    "            ax.set_title(name)\n",
    "        return plt\n",
    "    \n",
    "    \n",
    "    def plot_structure(self):\n",
    "        M = self.m.values\n",
    "        g = igraph.Graph.Adjacency((M != 0.0).tolist())\n",
    "        g.es['weight'] = M[M.nonzero()]\n",
    "        g.vs['label'] = self.nodes\n",
    "        g.vs['color'] = 'white'\n",
    "        g.vs['size'] = 45\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/rbc_100k.csv')\n",
    "data = data.drop(list(['eps_z', 'eps_g']), axis=1)\n",
    "data.columns = [col.replace(\" \", \"\") for col in data.columns]\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "data = data.apply(lambda x: x - x.mean())\n",
    "\n",
    "shift_vars = data.columns.values\n",
    "shift = data.loc[:,shift_vars].shift()\n",
    "shift.columns = [str(col) + '_1' for col in shift.columns]\n",
    "data = pd.concat([data, shift], axis=1)\n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "data_array = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_children = np.where(np.isin(data.columns.values, shift.columns.values))[0]\n",
    "bl_parents  = None \n",
    "d = data_array.shape[1]\n",
    "bnds = [\n",
    "        (0, 0)\n",
    "        if i == j\n",
    "        else (0, 0) \n",
    "        if bl_parents is not None and i in bl_parents\n",
    "        else (0, 0) \n",
    "        if bl_children is not None and j in bl_children\n",
    "        else (0, None) \n",
    "        for _ in range(2)\n",
    "        for i in range(d)\n",
    "        for j in range(d)\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(bnds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    results = pd.DataFrame(columns=['SHD', 'root_nodes', 'directed', 'dag', 'params'])\n",
    "    lambda1s = np.linspace(1e1, 1e-50, 10) \n",
    "    w_thresholds = np.linspace(0, 0.99, 10)\n",
    "    loss_types = ['l2', 'logistic', 'poisson']\n",
    "    bndss = [dag_bnds]\n",
    "    params = [lambda1s, w_thresholds, loss_types, bndss]\n",
    "    grid = list(itertools.product(*params))\n",
    "    gridsize = len(grid)\n",
    "    \n",
    "    i = 1\n",
    "    for p in grid:\n",
    "        W_est = linear.notears_linear(data_array, \n",
    "                                      lambda1=p[0], \n",
    "                                      w_threshold=p[1],\n",
    "                                      loss_type=p[2], \n",
    "                                      bnds=p[3],\n",
    "                                      max_iter=1000, \n",
    "                                      h_tol=1e-8, \n",
    "                                      rho_max=1e+20)\n",
    "        d = dag(pd.DataFrame(W_est, index=data.columns.values, columns=data.columns.values))\n",
    "        shd = d.shd(gt)\n",
    "        root_nodes = d.root_nodes()\n",
    "        directed = d.directed\n",
    "\n",
    "        results = results.append({\n",
    "            'SHD': shd,\n",
    "            'root_nodes': root_nodes,\n",
    "            'directed': directed,\n",
    "            'dag': d,\n",
    "            'params': {\n",
    "                'lambda1': p[0],\n",
    "                'w_threshold': p[1],\n",
    "                'loss_type': p[2],\n",
    "                'bnds': p[3]\n",
    "            }\n",
    "         }, ignore_index=True)\n",
    "        print('Completed iteration {} of {} ({}%)\\n SHD: {}'.format(i, gridsize, np.round(i/gridsize*100, 2), shd))\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "    print('FINISHED')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[results['directed']].sort_values(by='SHD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_est = linear.notears_linear(data_array, \n",
    "                              lambda1=1e-1, \n",
    "                              w_threshold=0.001,\n",
    "                              loss_type='poisson', \n",
    "                              max_iter=1000, \n",
    "                              h_tol=1e-30, \n",
    "                              rho_max=1e+30,\n",
    "                              bnds=bnds)\n",
    "                              # verbose=True)\n",
    "adj_df = pd.DataFrame(W_est, index=data.columns.values, columns=data.columns.values)\n",
    "d = dag(adj_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = d.plot_structure()\n",
    "igraph.plot(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(data.columns)\n",
    "gt = pd.DataFrame(np.zeros((d, d)), columns=data.columns, index=data.columns)\n",
    "\n",
    "gt.loc['z','y'] = 1.37278195\n",
    "gt.loc['g','y'] = 0.15452990\n",
    "gt.loc['k_1','y'] = 0.01074088\n",
    "\n",
    "gt.loc['z','c'] = 0.35193460\n",
    "gt.loc['g','c'] = -0.10362034\n",
    "gt.loc['k_1','c'] = 0.03140616\n",
    "\n",
    "gt.loc['z','k'] = 1.01252958\n",
    "gt.loc['g','k'] = 0.04465323\n",
    "gt.loc['k_1','k'] = 0.95566049\n",
    "\n",
    "gt.loc['z','l'] = 0.154009373\n",
    "gt.loc['g','l'] = 0.072779801\n",
    "gt.loc['k_1','l'] = -0.009885726\n",
    "\n",
    "gt.loc['z','r'] = 0.16661011\n",
    "gt.loc['g','r'] = 0.01875479\n",
    "gt.loc['k_1','r'] = -0.01036630\n",
    "\n",
    "gt.loc['z','w'] = 1.79625183\n",
    "gt.loc['g','w'] = -0.15452990\n",
    "gt.loc['k_1','w'] = 0.08541297\n",
    "\n",
    "gt.loc['z','i'] = 1.02084736\n",
    "gt.loc['g','i'] = 0.04502005\n",
    "gt.loc['k_1','i'] = -0.02066529\n",
    "\n",
    "gt.loc['z_1','z'] = 0.9702133\n",
    "gt.loc['g_1','g'] = 0.989444\n",
    "\n",
    "gt_dag = dag(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gt_dag.plot_structure()\n",
    "igraph.plot(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = pd.Series(np.full(len(gt_dag.nodes), np.nan), index=gt_dag.nodes)\n",
    "shock_amt = 0.66\n",
    "x_0[4] = shock_amt\n",
    "x_0[9:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_irf = gt_dag.calculate_irf(x_0, T=250)\n",
    "plt = gt_dag.plot_irf(irf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "est_irf = d.calculate_irf(x_0, T=250, verbose = True)\n",
    "plt = d.plot_irf(est_irf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([d.depth(n) for n in d.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, linalg\n",
    "from itertools import chain, combinations, product\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/real_data.csv')\n",
    "data = data.drop(list(['DATE']), axis=1)\n",
    "data.columns = [col.replace(\" \", \"\") for col in data.columns]\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "# data = data.iloc[:1000,:]\n",
    "data = data.apply(lambda x: x - x.mean())\n",
    "# data = data.applymap(lambda x: x + np.random.normal(scale=1))\n",
    "\n",
    "shift_vars = data.columns.values\n",
    "shift = data.loc[:,shift_vars].shift()\n",
    "shift.columns = [str(col) + '_1' for col in shift.columns]\n",
    "data = pd.concat([data, shift], axis=1)\n",
    "data = data.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation(y, x, z, data):\n",
    "    if len(z) == 0:\n",
    "        pcorr = stats.pearsonr(data[y], data[x])[0]\n",
    "    else:\n",
    "        model_y = LinearRegression(fit_intercept=False, normalize=True)\n",
    "        model_x = LinearRegression(fit_intercept=False, normalize=True)\n",
    "        model_y.fit(data[z], data[y])\n",
    "        model_x.fit(data[z], data[x])\n",
    "        resid_y = data[y] - model_y.predict(data[z])\n",
    "        resid_x = data[x] - model_x.predict(data[z])\n",
    "    \n",
    "        pcorr = stats.pearsonr(resid_y, resid_x)[0]\n",
    "    \n",
    "    return pcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    pset = list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "    variables = iterable[:np.int64(len(iterable)/2)]\n",
    "    pairs = {}\n",
    "    for v in variables:\n",
    "        pairs[v] = v+'_1'\n",
    "        pairs[v+'_1'] = v\n",
    "    pset = [s for s in pset if all([pairs[n] not in s for n in s])]\n",
    "    return pset\n",
    "        \n",
    "\n",
    "def choose_states(data, verbose=False):\n",
    "    lags = data.columns.values[np.int64(len(data.columns.values)/2):]\n",
    "    results = pd.DataFrame(columns=['states', 'mean_pcorr', 'min_pcorr', 'max_pcorr', 'pcorrs'])\n",
    "    for states in powerset(data.columns.values):\n",
    "        controls = [n for n in data.columns.values if n not in states and n not in lags]\n",
    "        if len(controls) < 2:\n",
    "            break\n",
    "        if verbose: \n",
    "            print('Calculating partial correlations for {}'.format(list(states)))\n",
    "        pcorrs = []\n",
    "        for p in product(controls, repeat=2):\n",
    "            pcorr = partial_correlation(p[0], p[1], list(states), data=data)\n",
    "            entry = {'y': p[0],\n",
    "                     'x': p[1],\n",
    "                     'z': states,\n",
    "                     'pcorr': pcorr}\n",
    "            pcorrs.append(entry)\n",
    "        result = {'states': list(states),\n",
    "                  'mean_pcorr': np.mean(np.abs([p['pcorr'] for p in pcorrs])),\n",
    "                  'min_pcorr': min([p['pcorr'] for p in pcorrs]),\n",
    "                  'max_pcorr': max([p['pcorr'] for p in pcorrs]),\n",
    "                  'pcorrs': pcorrs}\n",
    "        results = results.append(result, ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = choose_states(data, verbose=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nstates'] = results['states'].apply(lambda x: len(x))\n",
    "results.sort_values(by=['mean_pcorr', 'nstates'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['mean_pcorr'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(np.abs(results['min_pcorr']) > 0.999) & (results['max_pcorr'] > 0.999)].sort_values(by='mean_pcorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['score'] = results['mean_pcorr'] / len(results['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.iloc[:,:9]\n",
    "X = data.iloc[:,9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.1, max_iter=10000, fit_intercept = False, normalize = True)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is coefs for y_i\n",
    "lasso_results = pd.DataFrame(model.coef_, columns=X.columns, index=Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_results.loc[:,(lasso_results != 0).any()].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_grid_search(X, Y, a_min=-5, a_max=5, T=100, verbose=True, **kwargs):\n",
    "    results = pd.DataFrame(columns=['alpha', 'states', 'coefs', 'model'])\n",
    "    i = 1\n",
    "    for alpha in np.logspace(a_min, a_max, T):\n",
    "        if verbose:\n",
    "            print('Beginning iteration {} of {} ({}%) --- alpha = {}'.format(i, T, (i/T)*100, alpha))\n",
    "        model = Lasso(alpha=alpha, **kwargs)\n",
    "        model.fit(X, Y)\n",
    "        coefs = pd.DataFrame(model.coef_, columns=X.columns, index=Y.columns)\n",
    "        states = coefs.loc[:,(coefs != 0).any()].columns.values\n",
    "        result = {'alpha': alpha, 'states': states, 'coefs': coefs, 'model': model}\n",
    "        results = results.append(result, ignore_index=True)\n",
    "        i += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_results = lasso_grid_search(X, Y, T=100, max_iter=100000, fit_intercept = False, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for states in lasso_results['states']:\n",
    "    print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_irf(model, x_0, states, lags, T=250, verbose=False):\n",
    "    if verbose:\n",
    "        print('Simulating irf...')\n",
    "    for lag in lags:\n",
    "        x_0[lag] = 0\n",
    "    irf = pd.DataFrame([self.impute(x_0)], columns=self.nodes)\n",
    "    for t in range(T-1):\n",
    "        if verbose:\n",
    "            print('Simulating t={} of {} ({}%)'.format(t+1, T, np.round(100 * ((t+1)/T), decimals=2)))\n",
    "        nr = pd.Series(np.full(len(self.nodes), np.nan), index=self.nodes)\n",
    "        for lag in self.lags:\n",
    "            nr[lag] = irf.iloc[-1,:][lag.rstrip('_1')]\n",
    "        nr = self.impute(nr)    \n",
    "        irf = irf.append(nr, ignore_index=True)\n",
    "    irf.loc[:,'t'] = range(T)\n",
    "    irf.set_index('t', inplace=True)\n",
    "    irf.drop(self.lags, axis=1, inplace=True)\n",
    "    return irf\n",
    "    \n",
    "\n",
    "def plot_irfs(states, lags, data, T=250, **kwargs):\n",
    "    Y = data[~np.union1d(states, lags)]\n",
    "    X = data[states]\n",
    "    model = LinearRegression(**kwargs)\n",
    "    model.fit(X, Y)\n",
    "    irf = pd.DataFrame(columns=data.columns)\n",
    "    irf \n",
    "    for t in range(T):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 250\n",
    "states = np.array(['k_1', 'g_1', 'z_1'])\n",
    "X_0 = np.array([[0, 0.66 , 0]]) # initial values of states\n",
    "controls = data.columns.values[:9]\n",
    "lags = data.columns.values[9:]\n",
    "# Note: For now assume states are lags (can relax later i.e. z_1 -> z -> controls)\n",
    "\n",
    "Y = data[controls]\n",
    "X = data[states]\n",
    "model = LinearRegression(fit_intercept=False, normalize=True)\n",
    "model.fit(X, Y)\n",
    "\n",
    "controls_init = pd.Series(model.predict(X_0).reshape(len(controls)), index=controls)\n",
    "lags_init = pd.Series(np.zeros(len(lags)), index=lags)\n",
    "for i in range(len(states)):\n",
    "    lags_init[states[i]] = X_0[0, i]\n",
    "irf = pd.DataFrame(controls_init.append(lags_init)).T\n",
    "for t in range(1, T):\n",
    "    L = irf.iloc[-1,:][controls]\n",
    "    L.index = lags\n",
    "    S = np.array([L[states]])\n",
    "    C = pd.Series(model.predict(S).reshape(len(controls)), index=controls)\n",
    "    irf = irf.append(C.append(L), ignore_index=True)\n",
    "    \n",
    "irf.loc[:,'t'] = range(T)\n",
    "irf.set_index('t', inplace=True)\n",
    "irf.drop(lags, axis=1, inplace=True)\n",
    "\n",
    "side = math.ceil(math.sqrt(len(irf.columns)))\n",
    "layout = (side, side)\n",
    "axes = irf.plot(subplots=True, layout=layout, \n",
    "                color=\"black\", legend=False)\n",
    "for ax, name in zip(axes.flatten(), irf.columns.values):\n",
    "    ax.axhline(y=0, color=\"red\")\n",
    "    ax.set_title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 250\n",
    "states = np.array(['k_1'])\n",
    "X_0 = np.array([[1]]) # initial values of states\n",
    "controls = data.columns.values[:9]\n",
    "lags = data.columns.values[9:]\n",
    "# Note: For now assume states are lags (can relax later i.e. z_1 -> z -> controls)\n",
    "\n",
    "Y = data[controls]\n",
    "X = data[states]\n",
    "model = LinearRegression(fit_intercept=False, normalize=True)\n",
    "model.fit(X, Y)\n",
    "\n",
    "controls_init = pd.Series(model.predict(X_0).reshape(len(controls)), index=controls)\n",
    "lags_init = pd.Series(np.zeros(len(lags)), index=lags)\n",
    "for i in range(len(states)):\n",
    "    lags_init[states[i]] = X_0[0, i]\n",
    "irf = pd.DataFrame(controls_init.append(lags_init)).T\n",
    "for t in range(1, T):\n",
    "    L = irf.iloc[-1,:][controls]\n",
    "    L.index = lags\n",
    "    S = np.array([L[states]])\n",
    "    C = pd.Series(model.predict(S).reshape(len(controls)), index=controls)\n",
    "    irf = irf.append(C.append(L), ignore_index=True)\n",
    "    \n",
    "irf.loc[:,'t'] = range(T)\n",
    "irf.set_index('t', inplace=True)\n",
    "irf.drop(lags, axis=1, inplace=True)\n",
    "\n",
    "side = math.ceil(math.sqrt(len(irf.columns)))\n",
    "layout = (side, side)\n",
    "axes = irf.plot(subplots=True, layout=layout, \n",
    "                color=\"black\", legend=False)\n",
    "for ax, name in zip(axes.flatten(), irf.columns.values):\n",
    "    ax.axhline(y=0, color=\"red\")\n",
    "    ax.set_title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = pd.Series(np.full(len(gt_dag.nodes), np.nan), index=gt_dag.nodes)\n",
    "shock_amt = 1.04\n",
    "x_0[5] = shock_amt\n",
    "x_0[9:] = 0\n",
    "tru_irf = gt_dag.calculate_irf(x_0, T=250)\n",
    "plt = gt_dag.plot_irf(irf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "\n",
    "def srivastava(data):\n",
    "    '''\n",
    "    Inputs:\n",
    "        data: np.ndarray\n",
    "            Residual correlation matrix\n",
    "    Performs:\n",
    "        Perform test T3* from Srivastava (2005) to test wheter\n",
    "        the corrleation matrix is diagonal\n",
    "    Returns:\n",
    "        float\n",
    "    '''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if p > 1:\n",
    "        S = np.cov(data.T)\n",
    "        a2_hat = np.sum(np.square(np.diag(S)))\n",
    "        a4_hat = np.sum(np.power(np.diag(S), 4))\n",
    "        a20_hat = (n/(p*(n+2)))*a2_hat\n",
    "        a40_hat = (1/p)*a4_hat\n",
    "        g3_hat = (n/(n-1))*((np.trace(np.dot(S,S))-(1/n)*(np.trace(S))**2)/(np.sum(np.square(np.diag(S)))))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')\n",
    "            T3_hat = (n/2)*((g3_hat-1)/np.sqrt(1-(1/p)*(a40_hat/(a20_hat**2))))\n",
    "            if np.isnan(T3_hat): # Could have sqrt of negative, replace as in Wang et al.\n",
    "                T3_hat = (n/2)*((g3_hat-1)/np.sqrt(1-(a4_hat/(a2_hat**2))))\n",
    "\n",
    "        return T3_hat\n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0        \n",
    "\n",
    "\n",
    "def schott(data):\n",
    "    '''\n",
    "    Inputs:\n",
    "        data: np.ndarray\n",
    "            Residual correlation matrix\n",
    "    Performs:\n",
    "        Perform test from Schott (2005) to test wheter\n",
    "        the corrleation matrix is diagonal\n",
    "    Returns:\n",
    "        float\n",
    "    '''\n",
    "    n = data.shape[0]\n",
    "    m = data.shape[1]\n",
    "    k = m*(m-1)/2\n",
    "    if m > 1:\n",
    "        R = np.corrcoef(data.T)\n",
    "        # for i,j in combinations(list(range(R.shape[0])), 2):\n",
    "        #     if np.abs(R[i,j]) == 1 and i != j:\n",
    "        #         R[i,j] == 0\n",
    "        t_nm = np.sum(np.square(np.triu(R, k=1))) - ((m*(m-1))/(2*n))\n",
    "        s_nm = (m*(m-1)*(n-1))/((n**2)*(n+2))\n",
    "\n",
    "        return t_nm/np.sqrt(s_nm)\n",
    "    \n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_resids(roles, data):\n",
    "    '''\n",
    "    Inputs:\n",
    "        roles: state_space_estimation.roles\n",
    "        data: pd.DataFrame\n",
    "    Performs:\n",
    "        Collect linear regression residuals from the model specified in roles\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray)\n",
    "    '''\n",
    "    # Use numpy indexing instead of pandas for large performance increase\n",
    "    # (At the expense of some increased code complexity)\n",
    "    data = data.values\n",
    "    \n",
    "    # Conditioning sets\n",
    "    cset = np.append(roles.lag_2_endo_states_idx, roles.lag_exo_states_idx)\n",
    "    # cset = np.append(roles.lag_endo_states_idx, roles.exo_states_idx)\n",
    "    \n",
    "    # Targets \n",
    "    tar = np.append(np.append(roles.lag_endo_states_idx, roles.lag_controls_idx), roles.exo_states_idx)\n",
    "    # tar = np.append(roles.endo_states_idx, roles.controls_idx)\n",
    "    \n",
    "    if cset.shape[0] > 0:\n",
    "        lm = LinearRegression(fit_intercept=True, normalize=False)\n",
    "        lm.fit(data[:,cset], data[:,tar])\n",
    "        resid = data[:,tar] - lm.predict(data[:,cset]) \n",
    "    else:\n",
    "        resid = data[:,tar]\n",
    "    \n",
    "    return resid\n",
    "\n",
    "\n",
    "def test(roles, data, method='schott', alpha=0.05, tol=1e-20):\n",
    "    '''\n",
    "    Inputs:\n",
    "        roles: state_space_estimation.roles\n",
    "            Model upon which to conduct constraint tests\n",
    "        data: pd.DataFrame\n",
    "        method: one of ('srivastava', 'schott')\n",
    "            Testing strategy to use\n",
    "        alpha: float\n",
    "            Significance level\n",
    "        tol: float\n",
    "            Tolerence, used for detecting near zero residuals \n",
    "            which make testing unstable\n",
    "    Performs:\n",
    "        Conduct constraint-based (partial correlation) tests on data\n",
    "        given the state-space model specified by roles and return all\n",
    "        tests in a dictionary (two test statistics, two p-values, and\n",
    "        overall decision 'valid')\n",
    "    Returns:\n",
    "        tests: dict\n",
    "    '''\n",
    "    valid = True\n",
    "    resid = get_resids(roles, data)\n",
    "    if method == 'srivastava':\n",
    "        t = srivastava(resid)\n",
    "        crit_val = stats.norm.ppf(1-(alpha)) # One-sided test\n",
    "        p = 1 - stats.norm.cdf(t)\n",
    "        if t > crit_val:\n",
    "            valid = False\n",
    "\n",
    "    elif method == 'schott':\n",
    "        t = schott(resid)\n",
    "        crit_val = stats.norm.ppf(1-(alpha/2)) # Two-sided test\n",
    "        p = 2*(1 - stats.norm.cdf(np.abs(t)))\n",
    "        if np.abs(t) > crit_val:\n",
    "            valid = False\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('method {} not found'.format(method))\n",
    "\n",
    "    return {'t': t, 'p': p, 'valid': valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/rbc.csv')\n",
    "# data.set_index('DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = roles(['z', 'g'], ['k'], ['y', 'c', 'l', 'r', 'w', 'i'], data.columns.values)\n",
    "sample = data.sample(1000, replace=True)\n",
    "print(test(model, sample, method='srivastava'))\n",
    "print(test(model, sample, method='schott'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cset = ['k_2', 'z_1', 'g_1']\n",
    "# cset = ['k_1', 'z', 'g']\n",
    "tar = ['k_1', 'y_1', 'c_1', 'l_1', 'r_1', 'w_1', 'i_1', 'z', 'g']\n",
    "# tar = ['k', 'y', 'c', 'l', 'r', 'w', 'i']\n",
    "resid = get_resids(model, data)\n",
    "pd.DataFrame(np.corrcoef(resid.T), columns=tar, index=tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(1000, replace=True)\n",
    "cset = ['k_2', 'z_1', 'g_1']\n",
    "tar = ['k_1', 'y_1', 'c_1', 'l_1', 'r_1', 'w_1', 'i_1', 'z', 'g']\n",
    "\n",
    "lm = LinearRegression(fit_intercept=True, normalize=False)\n",
    "lm.fit(sample.loc[:,cset], sample.loc[:,tar])\n",
    "resid = sample.loc[:,tar] - lm.predict(sample.loc[:,cset]) \n",
    "\n",
    "f = len(model.controls) + len(model.endo_states)\n",
    "np.var(resid.values[:,:f].flatten()) < 1e-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def potential_states(n_states):\n",
    "    '''\n",
    "    Inputs:\n",
    "        n_states: int\n",
    "    Performs: \n",
    "        Create a generator containing a state_space_estimation.roles objects\n",
    "        for every possible state space model given this data and n_states\n",
    "    Returns:\n",
    "        generator\n",
    "    '''\n",
    "    variables = data.columns.values[:int(len(data.columns.values)/2)]\n",
    "    limit = len(variables)-1 if n_states is None else n_states\n",
    "    exo_states = chain.from_iterable(combinations(variables, r) for r in range(limit+1))\n",
    "    for exo in exo_states:\n",
    "        y = [z for z in variables if z not in exo]\n",
    "        endo_states = combinations(y, limit-len(exo))\n",
    "        for endo in endo_states:\n",
    "            controls = [z for z in variables if z not in endo and z not in exo]\n",
    "            yield roles(exo, endo, controls, data.columns.values)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['exo_states', 'endo_states', 'controls', 't1', 't2', 'valid' ])\n",
    "models = potential_states(3)\n",
    "for model in models:\n",
    "    t1, t2, valid = test(model, data, method='srivastava', tol=1e-5)\n",
    "    results = results.append({'exo_states': model.exo_states,\n",
    "                              'endo_states': model.endo_states,\n",
    "                              'controls': model.controls,\n",
    "                              't1': t1,\n",
    "                              't2': t2,\n",
    "                              'valid': valid}, \n",
    "                             ignore_index=True)\n",
    "results[results['valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['exo_states', 'endo_states', 'controls', 't1', 't2', 'valid' ])\n",
    "models = potential_states(3)\n",
    "for model in models:\n",
    "    t1, t2, valid = test(model, data, method='schott', tol=1e-5)\n",
    "    results = results.append({'exo_states': model.exo_states,\n",
    "                              'endo_states': model.endo_states,\n",
    "                              'controls': model.controls,\n",
    "                              't1': t1,\n",
    "                              't2': t2,\n",
    "                              'valid': valid}, \n",
    "                             ignore_index=True)\n",
    "results[results['valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['exo_states', 'endo_states', 'controls', 't1', 't2', 'valid' ])\n",
    "nstates = 0\n",
    "while results.shape[0] == 0:\n",
    "    models = potential_states(nstates)\n",
    "    for model in models:\n",
    "        t1, t2, valid = test(model, data, method='custom', tol=1e-8)\n",
    "        if valid:\n",
    "            results = results.append({'exo_states': model.exo_states,\n",
    "                                      'endo_states': model.endo_states,\n",
    "                                      'controls': model.controls,\n",
    "                                      't1': t1,\n",
    "                                      't2': t2,\n",
    "                                      'valid': valid}, \n",
    "                                     ignore_index=True)\n",
    "        nstates += 1\n",
    "results[results['valid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['exo_states', 'endo_states', 'controls', 't1', 't2', 'valid' ])\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "n = 1000\n",
    "k = 1000\n",
    "alpha = 0.05\n",
    "rejected = 0\n",
    "for i in range(k):\n",
    "    sample1 = np.array([norm.rvs(size=n), norm.rvs(size=n), norm.rvs(size=n)]).T \n",
    "    sample2 = np.array([norm.rvs(size=n), norm.rvs(size=n), norm.rvs(size=n)]).T\n",
    "    t1, t2, valid = test_resids(sample1, sample2, method='srivastava')\n",
    "    # if not valid:\n",
    "    if t1 > norm.ppf(1-alpha):\n",
    "        rejected += 1\n",
    "    \n",
    "print(rejected/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 17 # total number of variables\n",
    "l = 17 # number of restricted variables\n",
    "k = (l/2)*(2*m-l-1)\n",
    "X = np.array([1 for i in range(m**2)]).reshape(m,-1)\n",
    "counted = np.sum(np.triu(X, k=1)) - np.sum(np.triu(X[:(m-l),:(m-l)], k=1))\n",
    "assert counted == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schott(data, tol=1e-5):\n",
    "    n = data.shape[0]\n",
    "    m = data.shape[1]\n",
    "    if m > 1:\n",
    "        R = np.corrcoef(data.T)\n",
    "        # Check for collinearity\n",
    "        if (np.abs(R) > 1-tol).all(): # Residuals are so small computation is impossible, but this\n",
    "                                      # is an indication we have found the correct model... \n",
    "            return 0\n",
    "        else:\n",
    "            t_nm = np.sum(np.square(np.triu(R, k=1))) - ((m*(m-1))/(2*n))\n",
    "            s_nm = (m*(m-1)*(n-1))/((n**2)*(n+2))\n",
    "            # print(t_nm)\n",
    "            # print(s_nm)\n",
    "\n",
    "        return t_nm/np.sqrt(s_nm)\n",
    "    \n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def custom(data, l, tol=1e-5):\n",
    "    n = data.shape[0]   # sample size\n",
    "    m = data.shape[1]   # total number of variables\n",
    "    k = (l/2)*(2*m-l-1) # number of elements of R considered\n",
    "    if m > 1:\n",
    "        R = np.corrcoef(data.T)\n",
    "        if (np.abs(R[(m-l):, (m-l):]) > 1-tol).all(): # Residuals are so small computation is impossible, but this\n",
    "                                                      # is an indication we have found the correct model... \n",
    "            return 0        \n",
    "        else:\n",
    "            t_nm = (np.sum(np.square(np.triu(R, k=1))) \n",
    "                    - np.sum(np.square(np.triu(R[:(m-l),:(m-l)], k=1)))\n",
    "                    - k/n)\n",
    "            s_nm = (2*k*(n-1))/((n**2)*(n+2))\n",
    "            # print(t_nm)\n",
    "            # print(s_nm)\n",
    "\n",
    "            return t_nm/np.sqrt(s_nm)\n",
    "    \n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(1000, replace=True)\n",
    "resid1, resid2 = get_resids(model, sample)\n",
    "print(custom(resid2, 2))\n",
    "print(schott(resid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = ['z_1', 'g_1', 'k', 'y', 'c', 'l', 'r', 'w', 'i']\n",
    "names2 = ['k_1', 'z', 'g']\n",
    "resid1, resid2 = get_resids(model, sample, ntests=4)\n",
    "R = np.corrcoef(resid1.T)\n",
    "pd.DataFrame(R, columns=names1, index=names1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(1000, replace=True)\n",
    "X = sample.loc[:,['k_1', 'z', 'g']]\n",
    "Y = sample.loc[:,['k', 'y', 'c', 'l', 'r', 'w', 'i']]\n",
    "lm = LinearRegression(fit_intercept=True, normalize=False)\n",
    "lm.fit(X, Y)\n",
    "resid = lm.predict(X) - Y\n",
    "np.var(resid.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom(data, l, tol=1e-5):\n",
    "    n = data.shape[0]   # sample size\n",
    "    m = data.shape[1]   # total number of variables\n",
    "    k = (l/2)*(2*m-l-1) # number of elements of R considered\n",
    "    if m > 1 and k > 0:\n",
    "        R = np.corrcoef(data.T)      \n",
    "        included = np.triu(R, k=1)\n",
    "        excluded = np.concatenate([np.concatenate([np.triu(R[:(m-l),:(m-l)], k=1), \n",
    "                                                   np.zeros(((m-l), l))], axis=1), \n",
    "                                   np.zeros((l,m))], axis=0)\n",
    "        t_nm = (np.sum(np.square(included - excluded))) - k/n\n",
    "        s_nm = (2*k*(n-1))/((n**2)*(n+2))\n",
    "        return t_nm/np.sqrt(s_nm)\n",
    "    \n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "cor = 0.8\n",
    "m = 6\n",
    "l = 2\n",
    "\n",
    "cov = np.identity(m-l)\n",
    "cov[cov == 0] = cor\n",
    "cor_sample = mn.rvs(cov=cov, size=n).reshape(n, -1)\n",
    "uncor_sample = mn.rvs(cov=np.identity(l), size=n).reshape(n, -1)\n",
    "sample = np.concatenate([cor_sample, uncor_sample], axis=1)\n",
    "\n",
    "print(custom(sample, l))\n",
    "print(schott(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cors = [0.1, 0.2, 0.5, 0.8]\n",
    "ms = [4, 8, 12]\n",
    "ls = [2, 4, 6, 10]\n",
    "n = 1000\n",
    "k = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for c, m, l in itertools.product(cors, ms, ls):\n",
    "    if l < m:\n",
    "        rejected = 0\n",
    "        for i in range(k):\n",
    "            cov = np.identity(m-l)\n",
    "            cov[cov == 0] = c\n",
    "            cor_sample = mn.rvs(cov=cov, size=n).reshape(n, -1)\n",
    "            uncor_sample = mn.rvs(cov=np.identity(l), size=n).reshape(n, -1)\n",
    "            sample = np.concatenate([cor_sample, uncor_sample], axis=1)\n",
    "\n",
    "            t = custom(sample, l)\n",
    "            # if not valid:\n",
    "            if np.abs(t) > norm.ppf(1-(alpha/2)):\n",
    "                rejected += 1\n",
    "\n",
    "        emp_alpha = rejected/k\n",
    "\n",
    "        result = {'cor': c, 'm': m, 'l': l, 'n': n, 'k': k, 'alpha': alpha, 'emperical_alpha': emp_alpha, 'delta': np.abs(alpha)-emp_alpha}\n",
    "        results = results.append(result, ignore_index=True)\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='delta', ascending=False).loc[:,['cor', 'm', 'l', 'delta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in itertools.product([1, 2], [3, 4], [5, 6]):\n",
    "    print(i, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 6\n",
    "l = 4\n",
    "n = 1000\n",
    "k = 1000\n",
    "alpha = 0.05\n",
    "rejected = 0\n",
    "for i in range(k):\n",
    "    cov = np.identity(l)\n",
    "    cov[cov == 0] = 0.5\n",
    "    cor_sample1 = mn.rvs(cov=cov, size=n)\n",
    "    cor_sample2 = mn.rvs(cov=cov, size=n)\n",
    "    uncor_sample1 = mn.rvs(cov=np.identity(m-l), size=n)\n",
    "    uncor_sample2 = mn.rvs(cov=np.identity(m-l), size=n)\n",
    "    sample1 = np.concatenate([cor_sample1, uncor_sample1], axis=1)\n",
    "    sample2 = np.concatenate([cor_sample2, uncor_sample2], axis=1)\n",
    "        \n",
    "    t1 = custom(sample1, l)\n",
    "    t2 = custom(sample2, l)\n",
    "    # if not valid:\n",
    "    if np.abs(t1) > norm.ppf(1-(alpha/4)) or np.abs(t2) > norm.ppf(1-(alpha/4)):\n",
    "        rejected += 1\n",
    "    \n",
    "print(rejected/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srivastava(data):\n",
    "    '''\n",
    "    Inputs:\n",
    "        data: np.ndarray\n",
    "            Residual correlation matrix\n",
    "    Performs:\n",
    "        Perform test T3* from Srivastava (2005) to test wheter\n",
    "        the corrleation matrix is diagonal\n",
    "    Returns:\n",
    "        float\n",
    "    '''\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if p > 1:\n",
    "        S = np.cov(data.T)\n",
    "        a2_hat = np.sum(np.square(np.diag(S)))\n",
    "        a4_hat = np.sum(np.power(np.diag(S), 4))\n",
    "        a20_hat = (n/(p*(n+2)))*a2_hat\n",
    "        a40_hat = (1/p)*a4_hat\n",
    "        g3_hat = (n/(n-1))*((np.trace(np.dot(S,S))-(1/n)*(np.trace(S))**2)/(np.sum(np.square(np.diag(S)))))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')\n",
    "            T3_hat = (n/2)*((g3_hat-1)/np.sqrt(1-(1/p)*(a40_hat/(a20_hat**2))))\n",
    "            if np.isnan(T3_hat): # Could have sqrt of negative, replace as in Wang et al.\n",
    "                T3_hat = (n/2)*((g3_hat-1)/np.sqrt(1-(a4_hat/(a2_hat**2))))\n",
    "\n",
    "        return T3_hat\n",
    "    else: # Test isn't meaningful, so do not exclude the model on this basis\n",
    "        return 0       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 9\n",
    "n = 100\n",
    "k = 1000\n",
    "cs = [0] # np.linspace(0, 0.2, 11)\n",
    "alpha = 0.05\n",
    "results = pd.DataFrame()\n",
    "for c in cs:\n",
    "    rejected = 0\n",
    "    for i in range(k):\n",
    "        cov = np.identity(m)\n",
    "        cov[cov == 0] = c\n",
    "        sample = mn.rvs(cov=cov, size=n)\n",
    "        t = srivastava(sample)\n",
    "        if np.abs(t) > norm.ppf(1-(alpha/2)):\n",
    "            rejected += 1\n",
    "    results = results.append({'correlation': c, 'power': rejected/k}, ignore_index=True)\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb as ncr\n",
    "count = 0\n",
    "for i in range(0, 3+1):\n",
    "    for j in range(0, i+1):\n",
    "        count += ncr(i,j)*ncr(9,i)*ncr(9-i,j)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr(3,1)*ncr(9,1)*ncr(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18 + 144 + 672"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
